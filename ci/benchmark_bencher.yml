include:
  - local: 'ci/base.yml'

.benchmark_model_stencils:
  stage: benchmark
  script:
    - curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
    - source "$HOME/.cargo/env"
    - cargo install --git https://github.com/finkandreas/bencher --branch main --locked --force bencher_cli
    - uv sync --no-dev --extra=dace --extra=io --extra=testing --extra=$ICON4PY_NOX_UV_CUSTOM_SESSION_EXTRAS --group=test
    - source .venv/bin/activate
    - uv pip install git+https://github.com/iomaganaris/pytest-memray.git@pytest_json
    - PR_ID=$(echo "${CI_COMMIT_BRANCH}" | grep -o 'pr[0-9]*' | grep -o '[0-9]*')
    - FEATURE_BRANCH=$(curl -s https://api.github.com/repos/C2SM/icon4py/pulls/$PR_ID | jq -r '.head.ref')
    - export GITHUB_EVENT_NAME=pull_request
    - export GITHUB_STEP_SUMMARY=$CI_PROJECT_DIR/step_summary.log
    - export GITHUB_SHA=$CI_COMMIT_SHA
    - export GITHUB_ACTIONS=true
    - export GITHUB_EVENT_PATH=$CI_PROJECT_DIR/event.json
    - BENCHER_RESULTS_JSON="results_${CI_COMMIT_SHA}_${BACKEND}_${GRID}.json"
    - PYTESTS_RESULTS_JSON="pytest_${BENCHER_RESULTS_JSON}"
    - RUNTIME_BENCHER_RESULTS_JSON="runtime_${BENCHER_RESULTS_JSON}"
    - MEMRAY_BENCHER_RESULTS_JSON="memray_${BENCHER_RESULTS_JSON}"
    - |
      echo "{\"pull_request\": {\"head\": {\"repo\": {\"full_name\": \"C2SM/icon4py\"}}}, \"repository\": {\"full_name\": \"C2SM/icon4py\"}, \"number\": $PR_ID}" > $CI_PROJECT_DIR/event.json
    # Run pytest benchmark to gather runtime statistics
    - pytest --benchmark-json ${PYTESTS_RESULTS_JSON} -v --benchmark-only --backend=$BACKEND --grid=$GRID ./model
    # Run pytest benchmark to gather memory high watermark by memray (cannot be combined with runtime due to memray runtime overhead)
    - pytest --benchmark-json ${MEMRAY_BENCHER_RESULTS_JSON} -v --benchmark-only --memray --backend=$BACKEND --grid=$GRID ./model
    # Combine runtime and memory statistics in Bencher Metric Format
    - jq -n '[inputs.benchmarks[] | {(.fullname): {latency: {value: .stats.median, lower_bound: .stats.min, upper_bound: .stats.max}}}] | add' $PYTESTS_RESULTS_JSON > $RUNTIME_BENCHER_RESULTS_JSON
    - |
      jq --slurpfile mem <(jq -n '[inputs.benchmarks[] | {(.fullname): {memory_high_watermark: {value: .extra_info.memory_high_watermark}}}] | add' ${MEMRAY_BENCHER_RESULTS_JSON}) '
        with_entries(
          if $mem[0][.key] != null then
            .value.memory_high_watermark = $mem[0][.key].memory_high_watermark
          else
            .
          end
        )' $RUNTIME_BENCHER_RESULTS_JSON > $BENCHER_RESULTS_JSON
    - |
      bencher run \
      --project "$BENCHER_PROJECT" \
      --token "$BENCHER_API_TOKEN" \
      --github-actions $CK_COMMENT_TOKEN \
      --branch "$FEATURE_BRANCH" \
      --start-point main \
      --start-point-clone-thresholds \
      --start-point-reset \
      --testbed "ci-runner:${SYSTEM_NAME}:${BACKEND}:${GRID}" \
      --err \
      --adapter json \
      --file $BENCHER_RESULTS_JSON
    - |
      if [[ "$PR_ID" != "" && "$BACKEND" == "gtfn_cpu" && "$GRID" == "icon_grid" ]]; then
        COMMENT_BODY="[Bencher Dashboard](https://bencher.dev/perf/icon4py)"
        curl -L \
          -X POST \
          -H "Accept: application/vnd.github+json" \
          -H "Authorization: token $CK_COMMENT_TOKEN" \
          -H "X-GitHub-Api-Version: 2022-11-28" \
          https://api.github.com/repos/C2SM/icon4py/issues/$PR_ID/comments \
          -d "{\"body\": \"$COMMENT_BODY\"}"
      fi
  parallel:
    matrix:
      - BACKEND: [gtfn_cpu, gtfn_gpu]
        GRID: [icon_grid, icon_grid_global]

# benchmark_bencher_x86_64:
#   extends: [.benchmark_model_stencils, .test_template_x86_64]

benchmark_bencher_aarch64:
  extends: [.benchmark_model_stencils, .test_template_aarch64]
