include:
  - local: 'ci/base.yml'

.benchmark_model_stencils:
  stage: benchmark
  script:
    - curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
    - source "$HOME/.cargo/env"
    - cargo install --git https://github.com/bencherdev/bencher --branch main --locked --force bencher_cli
    - uv sync --no-dev --extra=dace --extra=io --extra=testing --extra=$ICON4PY_NOX_UV_CUSTOM_SESSION_EXTRAS --group=test
    - source .venv/bin/activate
    - |
      bencher run \
      --project "$BENCHER_PROJECT" \
      --token "$BENCHER_API_TOKEN" \
      --branch main \
      --testbed "ci-runner:$SYSTEM_NAME" \
      --threshold-measure latency \
      --threshold-test t_test \
      --threshold-max-sample-size 64 \
      --threshold-upper-boundary 0.99 \
      --thresholds-reset \
      --err \
      --adapter python_pytest \
      --file results.json \
      "pytest --benchmark-json results.json -v --benchmark-only --backend=$BACKEND --grid=$GRID ./model"
    - cp results.json $CI_PROJECT_DIR/results_before.json
    - |
      jq ". | {\"data_stream\": {\"type\": \"logs\", \"dataset\": \"service.icon4py_bencher_baseline\", \"namespace\": \"alps\"}} + ." results.json > results_tmp.json && mv results_tmp.json results.json
    - cp results.json $CI_PROJECT_DIR/results_after.json
  parallel:
    matrix:
      - BACKEND: [gtfn_cpu, gtfn_gpu]
        GRID: [icon_grid, icon_grid_global]
  artifacts:
    paths:
      - results_before.json
      - results_after.json

# benchmark_bencher_baseline_x86_64:
#   extends: [.benchmark_model_stencils, .test_template_x86_64]

benchmark_bencher_baseline_aarch64:
  extends: [.benchmark_model_stencils, .test_template_aarch64]
